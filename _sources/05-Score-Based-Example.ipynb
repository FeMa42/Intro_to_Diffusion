{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Score-Based Generative Model for 2D Swiss Roll Dataset\n",
    "\n",
    "## Generate Swiss Roll Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn import init\n",
    "# from torch.nn import functional as F\n",
    "# import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import functools\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# Set the device \n",
    "device = 'mps' # 'cuda' or 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll_dataset(n_samples=1000, noise=0.0, random_state=None):\n",
    "    \"\"\"\n",
    "    Function to generate Swiss Roll dataset.\n",
    "\n",
    "    Parameters:\n",
    "    n_samples (int): The total number of points equally divided among classes.\n",
    "    noise (float): Standard deviation of Gaussian noise added to the data.\n",
    "    random_state (int): Determines random number generation for dataset creation. \n",
    "\n",
    "    Returns:\n",
    "    X (torch.Tensor): The generated samples.\n",
    "    t (torch.Tensor): The univariate position of the sample according to the main dimension of the points in the Swiss Roll.\n",
    "    \"\"\"\n",
    "    X, t = make_swiss_roll(n_samples, noise=noise, random_state=random_state)\n",
    "    X = X[:, [0, 2]]\n",
    "    # Scale and normalize to [-1,1]\n",
    "    X = (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "    X = 10 * X - 5\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.from_numpy(X).float()\n",
    "    t = torch.from_numpy(t).float()\n",
    "\n",
    "    return X, t\n",
    "\n",
    "# Generate Swiss Roll dataset\n",
    "X, t = generate_swiss_roll_dataset(n_samples=100000, noise=0.3, random_state=42)\n",
    "print(f\"Swiss Roll dataset: {X.shape}\")\n",
    "print(f\"Labels: {t.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to Plot 2d Swiss Roll dataset\n",
    "def plot_2d_swiss_roll(X, t=None, title=\"Swiss Roll data\"):\n",
    "    if t is None:\n",
    "        t = np.zeros(X.shape[0])\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.Spectral)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_2d_swiss_roll(X, t, title=\"Swiss Roll dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmeneting Score-Based Generative Model\n",
    "\n",
    "## Score Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "  # [\\sin(2\\pi \\omega t) ; \\cos(2\\pi \\omega t)]\n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed\n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Score Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(nn.Module):\n",
    "\n",
    "  def __init__(self, marginal_prob_std, input_dim=2, hidden_dim=512, output_dim=2, embed_dim=256):\n",
    "    super().__init__()\n",
    "    # Gaussian random feature embedding layer for time\n",
    "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "      nn.Linear(embed_dim, embed_dim),\n",
    "      nn.GELU()\n",
    "    )\n",
    "      \n",
    "    self.linear_model1 = nn.Sequential(\n",
    "      nn.Linear(input_dim, embed_dim),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.GELU()\n",
    "    )\n",
    "\n",
    "    self.linear_model2 = nn.Sequential(\n",
    "      nn.Linear(embed_dim, hidden_dim),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.GELU(),\n",
    "      \n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.GELU(),\n",
    "      \n",
    "      nn.Linear(hidden_dim, input_dim),\n",
    "    )\n",
    "\n",
    "    self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    h = self.linear_model2(self.linear_model1(x) + self.embed(t))/ self.marginal_prob_std(t)[:, None]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_prob_std(t, sigma):\n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "sigma = 25.0\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)\n",
    "    perturbed_x = x + z * std[:, None]\n",
    "    score = model(perturbed_x, random_t)\n",
    "    loss = torch.mean(torch.sum((score * std[:, None] + z)**2, dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameter \n",
    "input_dim = 2  # x, y coordinates\n",
    "hidden_dim = 512 # hidden dimension\n",
    "output_dim = 2  # Score for each coordinate\n",
    "embed_dim = 256  # Dimension of the Gaussian random feature embedding\n",
    "\n",
    "# marginal_prob_std, input_dim=2, hidden_dim=32, output_dim=2, embed_dim=16\n",
    "score_model = ScoreNet(marginal_prob_std_fn, input_dim, hidden_dim, output_dim, embed_dim)\n",
    "score_model = score_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the Training parameter and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True if you have a pre-trained model\n",
    "load_model = False \n",
    "if load_model:\n",
    "    ckpt = torch.load('score_ckpt.pth', map_location=device)\n",
    "    score_model.load_state_dict(ckpt)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 8192 # size of a mini-batch 8192 \n",
    "n_epochs =  50  # number of training epochs 40\n",
    "lr = 1e-3  # learning rate 1e-3\n",
    "# Loss function parameters\n",
    "eps = 1e-3 # epsilon for numerical stability 1e-5\n",
    "\n",
    "# Prepare the data loader\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "t_tensor = torch.tensor(t, dtype=torch.float32, device=device)\n",
    "data_loader = DataLoader(\n",
    "    list(zip(X_tensor, t_tensor)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "tqdm_epoch = trange(n_epochs)\n",
    "for epoch in tqdm_epoch:\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "    for x, y in data_loader:\n",
    "        # x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        loss = loss_fn(score_model, x, marginal_prob_std_fn, eps=eps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "        num_items += x.shape[0]\n",
    "    # Print the averaged training loss so far.\n",
    "    tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "    # Update the checkpoint after each epoch of training.\n",
    "    torch.save(score_model.state_dict(), 'score_ckpt.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the sampling function - predictor corrector sampling based on score SDE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_sampler(score_model,\n",
    "               marginal_prob_std,\n",
    "               diffusion_coeff,\n",
    "               batch_size=64,\n",
    "               num_steps=400,\n",
    "               snr=0.16,\n",
    "               device='mps',\n",
    "               eps=1e-5):\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, input_dim, device=device) * marginal_prob_std(t)[:, None]\n",
    "    time_steps = np.linspace(1., eps, num_steps)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "    x = init_x\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for time_step in time_steps:\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "            # Corrector step (Langevin MCMC)\n",
    "            grad = score_model(x, batch_time_step)\n",
    "            grad_norm = torch.norm(grad, dim=-1).mean()\n",
    "            noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "            langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
    "            x = x + langevin_step_size * grad + torch.sqrt(2 * langevin_step_size) * torch.randn_like(x)\n",
    "\n",
    "            # Predictor step (Euler-Maruyama)\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            x_mean = x + (g**2)[:, None] * score_model(x, batch_time_step) * step_size\n",
    "            x = x_mean + torch.sqrt(g**2 * step_size)[:, None] * torch.randn_like(x)\n",
    "            samples.append(x_mean.cpu().clone().numpy())\n",
    "    \n",
    "    samples = np.stack(samples, axis=1)\n",
    "    time_series_samples = np.swapaxes(samples, 0, 1)\n",
    "    # The last step does not include any noise\n",
    "    return x_mean, time_series_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - SDE based Euler Maruyama sampling (simpler but less accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The number of sampling steps.\n",
    "num_steps =  500\n",
    "\n",
    "def Euler_Maruyama_sampler(score_model,\n",
    "                           marginal_prob_std,\n",
    "                           diffusion_coeff,\n",
    "                           batch_size=64,\n",
    "                           num_steps=num_steps,\n",
    "                           device='mps',\n",
    "                           eps=1e-5):\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  init_x = torch.randn(batch_size, input_dim, device=device) * marginal_prob_std(t)[:, None]\n",
    "  time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "  step_size = time_steps[0] - time_steps[1]\n",
    "  x = init_x\n",
    "  with torch.no_grad():\n",
    "    for time_step in time_steps:\n",
    "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "\n",
    "      g = diffusion_coeff(batch_time_step)\n",
    "      x_mean = x + (g**2)[:, None] * score_model(x, batch_time_step) * step_size\n",
    "      x = x_mean + torch.sqrt(g**2 * step_size)[:, None] * torch.randn_like(x)\n",
    "  # Do not include any noise in the last sampling step.\n",
    "  return x_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the score model using predictor corrector sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps =  100 \n",
    "signal_to_noise_ratio = 0.015 \n",
    "eps = 1e-3 \n",
    "num_samples = 2000 \n",
    "\n",
    "# Test the prdictor corrector sample function\n",
    "last_sample, sample_time_series = pc_sampler(score_model, marginal_prob_std_fn, diffusion_coeff_fn, num_steps=num_steps, batch_size=num_samples, device=device, snr=signal_to_noise_ratio, eps=eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_range = [-7, 7]\n",
    "# only take the last xx% of the samples for visualization\n",
    "# num_step = sample_time_series.shape[0]\n",
    "# sample_time_series = sample_time_series[int(num_step*0.1):, :, :]\n",
    "sample = torch.tensor(sample_time_series, dtype=torch.float32, device=device)\n",
    "def update_plot(i, data, scat):\n",
    "    scat.set_offsets(data[i].detach().cpu().numpy())\n",
    "    return scat\n",
    "\n",
    "numframes = len(sample)\n",
    "scatter_point = sample[0].detach().cpu().numpy()\n",
    "scatter_x, scatter_y = scatter_point[:,0], scatter_point[:,1]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.xlim(scatter_range)\n",
    "plt.ylim(scatter_range)\n",
    "scat = plt.scatter(scatter_x, scatter_y, s=1)\n",
    "plt.show()\n",
    "clear_output()\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_plot, frames=range(numframes), fargs=(sample, scat), interval=150)\n",
    "writergif = animation.PillowWriter(fps=50) \n",
    "\n",
    "ani.save('score_swiss.gif', writer=writergif)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODE based sampling - We can use powerful ODE solvers to sample from our score based model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "## The error tolerance for the black-box ODE solver\n",
    "error_tolerance = 1e-5 \n",
    "def ode_sampler(score_model,\n",
    "                marginal_prob_std,\n",
    "                diffusion_coeff,\n",
    "                batch_size=64,\n",
    "                atol=error_tolerance,\n",
    "                rtol=error_tolerance,\n",
    "                device='cuda',\n",
    "                z=None,\n",
    "                eps=1e-3):\n",
    "  \"\"\"Generate samples from score-based models with black-box ODE solvers.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that returns the standard deviation\n",
    "      of the perturbation kernel.\n",
    "    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    atol: Tolerance of absolute errors.\n",
    "    rtol: Tolerance of relative errors.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    z: The latent code that governs the final sample. If None, we start from p_1;\n",
    "      otherwise, we start from the given z.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "  \"\"\"\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  # Create the latent code\n",
    "  if z is None:\n",
    "    init_x = torch.randn(batch_size, input_dim, device=device) * marginal_prob_std(t)[:, None]\n",
    "  else:\n",
    "    init_x = z\n",
    "\n",
    "  shape = init_x.shape\n",
    "\n",
    "  def score_eval_wrapper(sample, time_steps):\n",
    "    \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
    "    sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "    time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "    with torch.no_grad():\n",
    "      score = score_model(sample, time_steps)\n",
    "    return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "\n",
    "  def ode_func(t, x):\n",
    "    \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
    "    time_steps = np.ones((shape[0],)) * t\n",
    "    g = diffusion_coeff(torch.tensor(t, dtype=torch.float32)).cpu().numpy()\n",
    "    return  -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "\n",
    "  # Run the black-box ODE solver.\n",
    "  res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "  print(f\"Number of function evaluations: {res.nfev}\")\n",
    "  x = torch.tensor(res.y[:, -1], dtype=torch.float32, device=device).reshape(shape)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5 #@param {'type':'number'}\n",
    "num_samples = 2000 #@param {'type':'integer'}\n",
    "\n",
    "# ode_sampler(score_model,\n",
    "#                 marginal_prob_std,\n",
    "#                 diffusion_coeff,\n",
    "#                 batch_size=64,\n",
    "#                 atol=error_tolerance,\n",
    "#                 rtol=error_tolerance,\n",
    "#                 device='cuda',\n",
    "#                 z=None,\n",
    "#                 eps=1e-3)\n",
    "\n",
    "# Test the prdictor corrector sample function\n",
    "# sampler = pc_sampler # Euler_Maruyama_sampler, pc_sampler\n",
    "samples = ode_sampler(score_model, marginal_prob_std_fn, diffusion_coeff_fn, batch_size=num_samples, device=device, eps=eps)\n",
    "\n",
    "# Convert tensor to numpy array and plot generated samples\n",
    "samples_np = samples.cpu().detach().numpy()\n",
    "plot_2d_swiss_roll(samples_np, t=None, title=\"Generated data samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
